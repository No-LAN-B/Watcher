import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
# Import RandomForestRegressor as an alternative/improvement
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split # We'll use manual split first
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import os
import glob # To easily find all feature files

# Note: This project is partially a learning exercise so commenting why / how something works will be very common as I use those comments to learn and remember. 

# Config
# Directory where individual stock feature CSVs are saved (output from ticker_script.py)
INPUT_FEATURE_DIR = "data/complete" 
# Output directory for the trained model and scaler
MODEL_OUTPUT_DIR = "models" 
# List of feature columns the model should use for training
# IMPORTANT: This MUST match the features generated in app.py for prediction!
FEATURE_COLUMNS = [
    'Open', 
    'High', 
    'Low', 
    #'Close', # Exclude current close price for prediction
    'Volume', 
    'return', 
    'sma_10', 
    'vol_10', 
    'Close_Lag_1'
    # Add any other features consistently generated by engineer_features_for_stock
]
#TARGET_COLUMN = 'Target_Price' # The column we want to predict (Next Day's Close)
TARGET_COLUMN = 'Target_Change' # Predicting the change now instead of the price
TRAIN_TEST_SPLIT_RATIO = 0.8 # Use 80% of data for training, 20% for testing

def load_and_combine_data(input_dir: str) -> pd.DataFrame | None:
    """Loads all '*_complete.csv' files from the input directory and combines them."""
    all_files = glob.glob(os.path.join(input_dir, "*_complete.csv"))
    if not all_files:
        print(f"Error: No '*_complete.csv' files found in '{input_dir}'.")
        print("Please run 'ticker_script.py' for your desired stocks first.")
        return None
        
    df_list = []
    print(f"Found {len(all_files)} feature files. Loading...")
    for f in all_files:
        try:
            df = pd.read_csv(f, index_col='Date', parse_dates=True)
            # Optional: Add ticker symbol if needed later, though not used by generic model directly
            # ticker = os.path.basename(f).replace("_complete.csv", "")
            # df['Ticker'] = ticker 
            df_list.append(df)
            print(f"  Loaded {os.path.basename(f)} ({len(df)} rows)")
        except Exception as e:
            print(f"  Warning: Could not load or parse {f}. Error: {e}")
            
    if not df_list:
        print("Error: Failed to load any valid data.")
        return None
        
    # Combine all dataframes
    combined_df = pd.concat(df_list)
    combined_df.sort_index(inplace=True) # Ensure chronological order across all stocks
    print(f"\nCombined data has {len(combined_df)} total rows.")
    return combined_df

def main():
    """Loads data, trains generic model, evaluates, and saves artifacts."""
    
    # 1. Load and Combine Data
    df_engineered = load_and_combine_data(INPUT_FEATURE_DIR)
    if df_engineered is None:
        return

    # 2. Creation of Target Variable
    # ** CHANGE 2: Calculate the difference between next day's close and today's close **
    print(f"\nCreating target variable ('{TARGET_COLUMN}')...")
    if 'Close' not in df_engineered.columns:
        print("Error: 'Close' column is missing. Cannot create target.")
        return
    # Calculate the actual change: (Close price tomorrow) - (Close price today)
    df_engineered[TARGET_COLUMN] = df_engineered['Close'].shift(-1) - df_engineered['Close'] 
    print(f"Target '{TARGET_COLUMN}' created (Next Day Close - Current Day Close).")

    # 3. Ensure Drop NaN Rows
    initial_rows = len(df_engineered)
    df_complete = df_engineered.dropna()
    rows_dropped = initial_rows - len(df_complete)
    if df_complete.empty:
        print("Error: DataFrame is empty after dropping NaNs. Check feature calculations or data.")
        return
    print(f"Dropped {rows_dropped} rows containing NaNs.")
    print(f"Final dataset size for training/testing: {len(df_complete)} rows.")

    # 4. Select Features (X) and Target (y)
    print(f"\nSelecting features and target...")
    # Verify all specified feature columns exist in the final DataFrame
    missing_features = [col for col in FEATURE_COLUMNS if col not in df_complete.columns]
    if missing_features:
        print(f"Error: The final DataFrame is missing required feature columns: {missing_features}")
        print(f"Available columns: {df_complete.columns.tolist()}")
        return
        
    X = df_complete[FEATURE_COLUMNS]
    y = df_complete[TARGET_COLUMN]
    print(f"Selected {X.shape[1]} features.")

    # 5. Split Data (Time Series Split)
    print(f"\nSplitting data ({TRAIN_TEST_SPLIT_RATIO*100:.0f}% train / {(1-TRAIN_TEST_SPLIT_RATIO)*100:.0f}% test)...")
    split_index = int(len(X) * TRAIN_TEST_SPLIT_RATIO)
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]
    print(f"  Training data shape: {X_train.shape}, {y_train.shape}")
    print(f"  Testing data shape: {X_test.shape}, {y_test.shape}")

    # 6. Scale Features
    print("\nScaling features...")
    scaler = StandardScaler()
    # Fit the scaler ONLY on the training data
    X_train_scaled = scaler.fit_transform(X_train)
    # Transform both training and test data using the fitted scaler
    X_test_scaled = scaler.transform(X_test)
    print("Features scaled.")

    # 7. Train Linear Regression Model
    # ** CHANGE 3: Consider using RandomForestRegressor for potentially better results on change **
    # model = LinearRegression(n_jobs=-1) 
    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10, min_samples_split=10) 
    print(f"\nTraining {type(model).__name__} model...") # Print which model is being trained
    model.fit(X_train_scaled, y_train)
    print("Model training complete.")

    # keep linear regression, but prioritize random forest for now
    #print("\nTraining Linear Regression model...")
    #model = LinearRegression(n_jobs=-1) # Use all available CPU cores
    #model.fit(X_train_scaled, y_train)
    #print("Model training complete.")

    # 8. Evaluate Model
    print("\nEvaluating model on test data...")
    y_pred = model.predict(X_test_scaled)
    
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = mse**0.5 
    r2 = r2_score(y_test, y_pred)

    # General Evaluation Metrics
    print("\n--- Evaluation Metrics ---")
    print(f"Mean Absolute Error (MAE):  {mae:.4f}") 
    print(f"Mean Squared Error (MSE):   {mse:.4f}")
    print(f"Root Mean Squared Err (RMSE):{rmse:.4f}") 
    print(f"R-squared (R²):           {r2:.4f}") 
    print("--------------------------")

    # Accuracy Metrics
    print("\n--- Actual vs. Predicted Values (Test Set Sample) ---")
    # Create a DataFrame for easy comparison
    # Use the original index (Dates) from the test set
    comparison_df = pd.DataFrame({
        'Actual_Change': y_test, 
        'Predicted_Change': y_pred
        }, index=y_test.index)
    
    # Calculate the difference (error) for each prediction
    comparison_df['Prediction_Error'] = comparison_df['Actual_Change'] - comparison_df['Predicted_Change']
    
    # Display the first 5 and last 5 rows of the comparison
    print("Head:")
    print(comparison_df.head())
    print("\nTail:")
    print(comparison_df.tail())
    print("----------------------------------------------------")

    # Interpretation notes:
    # MAE/RMSE: Average error in price units. Lower is better.
    # R²: Proportion of variance explained. Closer to 1 is better. 
    # For stock prices, R² > 0 is often hard, R² near 1 is highly unlikely/suspicious.

    # --- 9. Save Model and Scaler ---
    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True) # Create output dir if needed
    model_path = os.path.join(MODEL_OUTPUT_DIR, "generic_linear_regression_model.pkl")
    scaler_path = os.path.join(MODEL_OUTPUT_DIR, "generic_scaler.pkl")
    
    print(f"\nSaving trained model to: {model_path}")
    joblib.dump(model, model_path)
    
    print(f"Saving fitted scaler to: {scaler_path}")
    joblib.dump(scaler, scaler_path)
    
    print("\nTraining script finished.")

if __name__ == "__main__":
    main()
